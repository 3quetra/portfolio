{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Naive Bayes Classifier\n",
    "\n",
    "The goal of this project is to build a simple Naive Bayes Classifier using `nltk toolkit`, and after that: train and test it on Movie Reviews corpora from `nltk.corpus`.\n",
    "\n",
    "Note: We will also use `pickle` to save the trained classifier, reducing the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Load classifier from pickle file\n",
    "if os.path.isfile('naivebayes.pickle'):\n",
    "    # Open existing pickle file for reading\n",
    "    trained_classifier_file = open('naivebayes.pickle', 'rb')\n",
    "    # Get trained classifier to work with it\n",
    "    trained_classifier = pickle.load(trained_classifier_file)\n",
    "    # Close the pickled file\n",
    "    trained_classifier_file.close()\n",
    "\n",
    "# Save object if not saved\n",
    "else:\n",
    "    \n",
    "    def reviews_words_lists(movie_reviews):\n",
    "        reviews_list = []\n",
    "        for category in movie_reviews.categories():\n",
    "            for file_id in movie_reviews.fileids(category):\n",
    "                reviews_list.append((list(movie_reviews.words(file_id)), category))\n",
    "        return reviews_list\n",
    "\n",
    "\n",
    "    # Get words from each review in separate lists marked by category\n",
    "    documents = reviews_words_lists(movie_reviews)\n",
    "\n",
    "\n",
    "    # Randomise review lists to mix positive and negative \n",
    "    random.shuffle(documents)\n",
    "\n",
    "\n",
    "    # Get all words from all reviews & make them lower case\n",
    "    all_words = [word.lower() for word in movie_reviews.words()]\n",
    "\n",
    "\n",
    "    # Count number of occurances for each word and sort in desc. order\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "    # Take 3000 most common from all available words\n",
    "    word_features = [w[0] for w in all_words.most_common(3000)]\n",
    "\n",
    "\n",
    "    def find_features(document, word_features):\n",
    "        words = set(document)\n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            features[word] = word in words\n",
    "        return features\n",
    "\n",
    "\n",
    "    # Mark whether or not each word in all reviews is also in the list of 3000 most common words\n",
    "    feature_sets = [(find_features(rev, word_features), category) for (rev, category) in documents]\n",
    "\n",
    "\n",
    "    # Take as training 1900 sets of words\n",
    "    training_sets = feature_sets[:1900]\n",
    "    # Take as training 100 sets of words\n",
    "    testing_sets = feature_sets[1900:]\n",
    "\n",
    "\n",
    "    # Train classifier on training sets\n",
    "    trained_classifier = nltk.NaiveBayesClassifier.train(training_sets)\n",
    "\n",
    "    # Create new file for classifier if not saved\n",
    "    save_trained_classifier = open('naivebayes.pickle', 'wb')\n",
    "    # Take contents of trained classifier and put it to the new file\n",
    "    pickle.dump(trained_classifier, save_trained_classifier)\n",
    "    # Close the file\n",
    "    save_trained_classifier.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our classifier on 1900 reviews, now let's check its accuracy on a testing set, consisting of 100 reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get classifier accuracy on testing sets\n",
    "nltk.classify.accuracy(trained_classifier, testing_sets)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: `78%` is rather high for this type of classifier, but the volatility of this algorithm is also very high. The result changes dramatically every launch becasue we randomise our sets, without optimising parameters or the dataset. Let's check what are the most informative features of the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.1 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.9 : 1.0\n",
      "                  seagal = True              neg : pos    =      8.3 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.5 : 1.0\n",
      "                  finest = True              pos : neg    =      7.5 : 1.0\n",
      "                  alicia = True              neg : pos    =      6.7 : 1.0\n",
      "              schumacher = True              neg : pos    =      6.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      6.6 : 1.0\n",
      "                   inept = True              neg : pos    =      6.1 : 1.0\n",
      "                     era = True              pos : neg    =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "trained_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: The are some decent examples of words intrinsic to positive reviews like: `outstanding`, `wonderfully`, `finest`. As well as negative:  `idiotic`, `inept`. \n",
    "But there are also some words that don't contain any connotation by themselves but still were considered characteristic of positive or negative reviews. For positive they are: `mulan`, `seagal`, `era`. For negative: `alicia`, `schumacher `. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a166e61b3f1e7b9ccc9f59f78d7cc3087a9ccbb0b1a7cff444b301613a633a54"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
